# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XBwcRtUVNBP6TXg_r9tIPj8RjXFAo2uU

#4. Training a Neural Network and logging to TensorBoard for visualisations

1. Importing required libraries and loading tensorboard extension in colab
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import datetime

"""2. Defining the log directory"""

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

"""3. Setting p tensorflow callback"""

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

"""4. Loading the MNIST dataset from tensorflow (x_train, x_test, y_train, y_test)
5. The grayscale images in the MNIST dataset have pixel values ranging from 0 to 255 (8 bit representation), Neural networks perform better when the inpute values scaled to a smaller range (between 0 to 1). Dividing by 255.0 scales the values to [0, 1], making the training process more stable (avaoding large weight updates), faster convergence (since values are in a small and manageable range), helps avoid issues elated to exploding or vanishing gradients.
"""

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

"""6. create_model() function creates and returns a tensorflow sequential mode.
i) sequential() to create a stack of layer where each layers output serves as an input to the next layer
ii) Flatten() to convert 2D image(28*28) into a 1D vector(784,)
iii) first hiddeen layer with 128 neurons, and relu activation function which helps in introducing non-linearity nd helping the network to learn complex patterns.
7. using create_model() function to reuse the same code for building independent models
"""

def create_model():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    return model

"""8.
Compiling the model ADAM(Adaptive moment estimation), sparse_categorical_crossentropy (best when lables are integers instead of one hot encoded values), accuracy (tracks model performance during training)
"""

model = create_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""9. Creates a tensorflow callback that logs training metrics, which can be visualised later on tensorboard, this callback function automatically logs loss values, acucuracy metrics, histogroms of layer weights and activation, Graph of the model architecture and Other useful training details."""

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir="logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S"), histogram_freq=1)

"""10. for simple datasets like MNIST, 5 epoch is usually sufficient for achieving a good accuracy (>90%), and verbose 2 is to display one line per epoch"""

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test),
          callbacks=[tensorboard_callback])

train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)

"""11. Printitng test loss and accuracy for both adam and sgd models"""

print(f"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}")

"""12. Running TensorBoard inside Google Colab"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/fit

"""Questions (4.1):
1. What patterns do you observe in the training and validation accuracy curves?
Ans: Training accuracy increases steadily as the model learns from the data, Validation accuracy follows a similar trend initially but it slightly decreases (remains constant) after some epochs.

2. How can you use TensorBoard to detect overfitting?
Overfitting: occurs when the model memorizes the training data but fails to generalize to unseen data.
Ans: signs of overfitting in tensorBoard are, Training accuracy keeps increasing, but validation accuracy plateaus or drops and Training loss continues to decrease, but validation loss starts rising.
Early stopping in TensorFlow: early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)

3. What happens when you increase the number of epochs?
Ans: If the model is not overfitting: accuracy continues improving and loss decreases gradually but, If the model is overfitting: Training accuracy reaches near 100%, while validation accuracy stops improving or drops, Training loss goes very low, but validation loss increases.
In overfitting case too many epochs leads to memorisation instead of learning.
"""